{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "\n",
    "X_t = train_data.drop(['category', 'ID'], axis=1)\n",
    "y_t = train_data['category']\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: Pipelining the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'metaclass' from '/Users/Atharv/Documents/uni/Semester 4/CSE342 - Statistical Machine Learning [SML]/Project/SML-Project/metaclass.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import metaclass as mc\n",
    "import pipeline_components as pc\n",
    "import importlib\n",
    "importlib.reload(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = ['Apple_Raw', 'Apple_Ripe', 'Banana_Raw', 'Banana_Ripe', 'Coconut_Raw',\n",
    "#  'Coconut_Ripe', 'Guava_Raw', 'Guava_Ripe', 'Leeche_Raw', 'Leeche_Ripe',\n",
    "#  'Mango_Raw', 'Mango_Ripe', 'Orange_Raw', 'Orange_Ripe', 'Papaya_Raw',\n",
    "#  'Papaya_Ripe', 'Pomengranate_Raw', 'Pomengranate_Ripe', 'Strawberry_Raw',\n",
    "#  'Strawberry_Ripe']\n",
    "\n",
    "# Get the category labels\n",
    "# labels = sorted(list(set(y_t)))\n",
    "\n",
    "# # Convert category labels to numbers\n",
    "# y_t_int = y_t.map(lambda x: labels.index(x))\n",
    "# print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing data...\n",
      "Currently at clustering: ('kmeans', 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Atharv/Library/Python/3.9/lib/python/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at dim reduction\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=119 must be between 0 and min(n_samples, n_features)=15 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline \u001b[39m=\u001b[39m mc\u001b[39m.\u001b[39mPipeline(\n\u001b[1;32m      2\u001b[0m     clustering_alg\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkmeans\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m20\u001b[39m),\n\u001b[1;32m      3\u001b[0m     dim_reduction_algs\u001b[39m=\u001b[39m[(\u001b[39m\"\u001b[39m\u001b[39mpca\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m119\u001b[39m)],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     ensemble_algs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(X_t, y_t)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPipeline done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# print(\"Pipeline score: \", pipeline.score(X_t, y_t))\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/uni/Semester 4/CSE342 - Statistical Machine Learning [SML]/Project/SML-Project/metaclass.py:53\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X_t, y_t)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m alg, n_components \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_reduction_algs:\n\u001b[1;32m     52\u001b[0m         dr \u001b[39m=\u001b[39m pc\u001b[39m.\u001b[39mDimReduction(alg, n_components)\n\u001b[0;32m---> 53\u001b[0m         X_t \u001b[39m=\u001b[39m dr\u001b[39m.\u001b[39;49mtransform(X_t, y_t)\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_red_objs\u001b[39m.\u001b[39mappend(dr)\n\u001b[1;32m     56\u001b[0m \u001b[39m# outlier detection\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/uni/Semester 4/CSE342 - Statistical Machine Learning [SML]/Project/SML-Project/pipeline_components.py:110\u001b[0m, in \u001b[0;36mDimReduction.transform\u001b[0;34m(self, X_t, y_t)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X_t: np\u001b[39m.\u001b[39mndarray, y_t: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapping[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malgorithm](X_t, y_t)\n",
      "File \u001b[0;32m~/Documents/uni/Semester 4/CSE342 - Statistical Machine Learning [SML]/Project/SML-Project/pipeline_components.py:116\u001b[0m, in \u001b[0;36mDimReduction.pca\u001b[0;34m(self, X_t, y_t)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[1;32m    115\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components)\n\u001b[0;32m--> 116\u001b[0m X_t_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(X_t)  \n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_red \u001b[39m=\u001b[39m pca\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m X_t_pca\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/decomposition/_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 462\u001b[0m U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    463\u001b[0m U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/decomposition/_pca.py:512\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_full(X, n_components)\n\u001b[1;32m    513\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_truncated(X, n_components, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/decomposition/_pca.py:526\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    523\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is only supported if n_samples >= n_features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m         )\n\u001b[1;32m    525\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_components \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m must be between 0 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmin(n_samples, n_features)=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msvd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_components, \u001b[39mmin\u001b[39m(n_samples, n_features))\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[39m# Center data\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=119 must be between 0 and min(n_samples, n_features)=15 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = mc.Pipeline(\n",
    "    clustering_alg=None,\n",
    "    dim_reduction_algs=[(\"pca\", 119)],\n",
    "    outlier_detection_alg=None,\n",
    "    classification_alg=\"logistic\",\n",
    "    ensemble_algs=None,\n",
    ")\n",
    "pipeline.fit(X_t, y_t)\n",
    "print(\"Pipeline done\")\n",
    "# print(\"Pipeline score: \", pipeline.score(X_t, y_t))\n",
    "cv_scores = pipeline.cross_validate(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7911623827835121, 0.032399689396669776)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv_scores.sort()\n",
    "# 100 pca, 1 lda, logistic, no ensemble\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.generate_submission(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
